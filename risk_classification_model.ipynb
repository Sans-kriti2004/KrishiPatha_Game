{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RISK CLASSIFICATION MODEL (Model E)\n",
        "Predict crop risk level (Low/Medium/High) using Random Forest:\n",
        "- Scratch implementation (educational)\n",
        "- scikit-learn implementation (production)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 1-12: Import all necessary libraries\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier  # NOTE: Classifier, not Regressor!\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Classification metrics\n",
        "import joblib\n",
        "import random\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully. Shape: (2438, 18)\n",
            "Columns: ['field_id', 'latitude', 'longitude', 'year', 'season_number', 'season_start', 'season_end', 'season_length_days', 'yield_kg_ha', 'ndvi_peak', 'ndvi_mean', 'ndvi_auc', 'ndvi_slope_mid', 'precip_cum', 'rh_mean', 'gdd_cum', 'solar_cum', 'water_stress_index']\n"
          ]
        }
      ],
      "source": [
        "# Line 13-25: Load the dataset\n",
        "DATAFILE = \"global_large_12year_yield_dataset.csv\"  # Using existing dataset\n",
        "try:\n",
        "    df = pd.read_csv(DATAFILE)\n",
        "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: {DATAFILE} not found. Place dataset in this folder.\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['ws_norm'] = (df['water_stress_index'] - df['water_stress_index'].min()) / \\\n",
        "                (df['water_stress_index'].max() - df['water_stress_index'].min())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 26-38: Create risk categories from water_stress_index using discrete risk classes\n",
        "# This function converts water stress index (0-1) into discrete risk classes\n",
        "def categorize_risk(ws_norm):\n",
        "    # Example thresholds, you can tune\n",
        "    if ws_norm < 0.33:\n",
        "        return 0  # Low\n",
        "    elif ws_norm  < 0.66:\n",
        "        return 1  # Medium\n",
        "    else:\n",
        "        return 2  # High\n",
        "\n",
        "if 'water_stress_index' in df.columns:\n",
        "    df['risk_class'] = df['water_stress_index'].apply(categorize_risk)  # Apply categorization\n",
        "else:\n",
        "    print(\"ERROR: No 'water_stress_index' column found to categorize risk.\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 39-50: Prepare features and target for classification\n",
        "target_col = 'risk_class'  # Our categorical target (0, 1, 2)\n",
        "\n",
        "# Select numeric features (excluding target)\n",
        "feature_cols = [c for c in df.columns if c != target_col and np.issubdtype(df[c].dtype, np.number)]\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No numeric features detected for classification.\")\n",
        "\n",
        "X = df[feature_cols].values  # Feature matrix\n",
        "y = df[target_col].values    # Target vector (categorical)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 51-55: Scale features (optional for Random Forest but good practice)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 56-67: Scratch Random Forest Classifier - Node class\n",
        "class ScratchTreeNode:\n",
        "    __slots__ = ('pred', 'feat', 'thresh', 'left', 'right')\n",
        "    def __init__(self, pred=None, feat=None, thresh=None, left=None, right=None):\n",
        "        self.pred = pred  # majority class at leaf (for classification)\n",
        "        self.feat = feat\n",
        "        self.thresh = thresh\n",
        "        self.left = left\n",
        "        self.right = right\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 68-75: Gini impurity function for classification splits\n",
        "# This measures how \"mixed\" the classes are in a node\n",
        "def gini_loss(y):\n",
        "    if len(y) == 0:\n",
        "        return 0.0\n",
        "    classes, counts = np.unique(y, return_counts=True)  # Count each class\n",
        "    probs = counts / counts.sum()  # Calculate probabilities\n",
        "    return float(1 - np.sum(probs ** 2))  # Gini = 1 - sum(pÂ²)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 76-95: Find best split using Gini impurity (not MSE like regression)\n",
        "def best_split_quick(X, y, feature_indices, n_thresholds=10, min_samples_leaf=3):\n",
        "    n, d = X.shape\n",
        "    best_feat, best_thresh, best_score = None, None, float('inf')\n",
        "    for feat in feature_indices:\n",
        "        col = X[:, feat]\n",
        "        if np.all(col == col[0]):  # Skip if all values are same\n",
        "            continue\n",
        "        thresholds = np.percentile(col, np.linspace(5,95,n_thresholds))  # Candidate thresholds\n",
        "        for thresh in thresholds:\n",
        "            left_mask = col <= thresh\n",
        "            right_mask = ~left_mask\n",
        "            if left_mask.sum() < min_samples_leaf or right_mask.sum() < min_samples_leaf:\n",
        "                continue\n",
        "            score = gini_loss(y[left_mask]) + gini_loss(y[right_mask])  # Combined Gini\n",
        "            if score < best_score:\n",
        "                best_feat = feat\n",
        "                best_thresh = float(thresh)\n",
        "                best_score = float(score)\n",
        "    return best_feat, best_thresh, best_score if best_feat is not None else (None, None, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 96-116: Build decision tree for classification\n",
        "def build_scratch_tree(X, y, depth=0, max_depth=6, min_samples_leaf=5):\n",
        "    # Stopping conditions\n",
        "    if depth >= max_depth or len(y) <= 2*min_samples_leaf or np.all(y == y[0]):\n",
        "        counts = np.bincount(y)  # Count occurrences of each class\n",
        "        pred_class = np.argmax(counts)  # Majority class\n",
        "        return ScratchTreeNode(pred=pred_class)\n",
        "    \n",
        "    m, d = X.shape\n",
        "    k = max(1, int(math.sqrt(d)))  # Random feature subset\n",
        "    features = np.random.choice(d, k, replace=False)\n",
        "    \n",
        "    feat, thresh, score = best_split_quick(X, y, features, n_thresholds=8, min_samples_leaf=min_samples_leaf)\n",
        "    if feat is None:  # No good split found\n",
        "        counts = np.bincount(y)\n",
        "        pred_class = np.argmax(counts)\n",
        "        return ScratchTreeNode(pred=pred_class)\n",
        "    \n",
        "    # Recursively build left and right subtrees\n",
        "    left_idx = X[:, feat] <= thresh\n",
        "    right_idx = ~left_idx\n",
        "    left = build_scratch_tree(X[left_idx], y[left_idx], depth+1, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
        "    right = build_scratch_tree(X[right_idx], y[right_idx], depth+1, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
        "    return ScratchTreeNode(pred=None, feat=feat, thresh=thresh, left=left, right=right)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 117-125: Predict using a single tree\n",
        "def predict_tree(node, x):\n",
        "    while node.pred is None:  # Traverse until leaf\n",
        "        if x[node.feat] <= node.thresh:\n",
        "            node = node.left\n",
        "        else:\n",
        "            node = node.right\n",
        "    return node.pred  # Return class prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line 126-150: Scratch Random Forest Classifier class\n",
        "class ScratchRandomForestClassifier:\n",
        "    def __init__(self, n_trees=10, max_depth=6, min_samples_leaf=5, bootstrap=True, random_state=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.trees = []\n",
        "        if random_state is not None:\n",
        "            random.seed(random_state)\n",
        "            np.random.seed(random_state)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]\n",
        "        self.trees = []\n",
        "        for t in range(self.n_trees):\n",
        "            idx = np.random.choice(n, n, replace=True) if self.bootstrap else np.arange(n)\n",
        "            tree = build_scratch_tree(X[idx], y[idx], max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n",
        "            self.trees.append(tree)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        preds = np.zeros((len(self.trees), X.shape[0]), dtype=int)  # Class predictions\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            preds[i] = np.array([predict_tree(tree, x) for x in X])\n",
        "        # Majority vote across trees\n",
        "        return np.apply_along_axis(lambda row: np.bincount(row).argmax(), axis=0, arr=preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training Scratch Random Forest Classifier ===\n",
            "Scratch RF training time: 0.22s\n",
            "Scratch RF accuracy: 0.973\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       456\n",
            "           1       1.00      0.59      0.75        32\n",
            "\n",
            "    accuracy                           0.97       488\n",
            "   macro avg       0.99      0.80      0.87       488\n",
            "weighted avg       0.97      0.97      0.97       488\n",
            "\n",
            "Confusion matrix:\n",
            " [[456   0]\n",
            " [ 13  19]]\n"
          ]
        }
      ],
      "source": [
        "# Line 151-165: Train scratch Random Forest classifier\n",
        "print(\"\\n=== Training Scratch Random Forest Classifier ===\")\n",
        "start_time = time.time()\n",
        "scratch_rf = ScratchRandomForestClassifier(n_trees=12, max_depth=7, min_samples_leaf=5, random_state=42)\n",
        "scratch_rf.fit(X_train, y_train)\n",
        "t_scratch_train = time.time() - start_time\n",
        "print(f\"Scratch RF training time: {t_scratch_train:.2f}s\")\n",
        "\n",
        "y_pred_scratch = scratch_rf.predict(X_test)\n",
        "acc_scratch = accuracy_score(y_test, y_pred_scratch)  # Classification accuracy\n",
        "print(f\"Scratch RF accuracy: {acc_scratch:.3f}\")\n",
        "print(classification_report(y_test, y_pred_scratch))  # Detailed classification metrics\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_scratch))\n",
        "\n",
        "with open(\"rf_risk_scratch.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scratch_rf, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training scikit-learn RandomForestClassifier ===\n",
            "sklearn RF accuracy: 1.000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       456\n",
            "           1       1.00      1.00      1.00        32\n",
            "\n",
            "    accuracy                           1.00       488\n",
            "   macro avg       1.00      1.00      1.00       488\n",
            "weighted avg       1.00      1.00      1.00       488\n",
            "\n",
            "Confusion matrix:\n",
            " [[456   0]\n",
            " [  0  32]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['rf_risk_sklearn.joblib']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Line 166-179: Train scikit-learn RandomForestClassifier\n",
        "print(\"\\n=== Training scikit-learn RandomForestClassifier ===\")\n",
        "start_time = time.time()\n",
        "rf_sklearn = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)\n",
        "rf_sklearn.fit(X_train_scaled, y_train)\n",
        "t_sklearn_train = time.time() - start_time\n",
        "y_pred_sklearn = rf_sklearn.predict(X_test_scaled)\n",
        "acc_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"sklearn RF accuracy: {acc_sklearn:.3f}\")\n",
        "print(classification_report(y_test, y_pred_sklearn))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_sklearn))\n",
        "\n",
        "joblib.dump({'model': rf_sklearn, 'scaler': scaler, 'features': feature_cols}, \"rf_risk_sklearn.joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example JSON output for risk model:\n",
            "{'risk_class': 0, 'risk_probs': [0.68, 0.32, 0.0]}\n",
            "\n",
            "============================================================\n",
            "KEY DIFFERENCES FROM REGRESSION MODEL:\n",
            "============================================================\n",
            "1. Uses RandomForestClassifier (not Regressor)\n",
            "2. Target is categorical (0, 1, 2) not continuous\n",
            "3. Uses Gini impurity for splits (not MSE)\n",
            "4. Uses accuracy, classification_report (not RMSE, RÂ²)\n",
            "5. Predicts class probabilities (not continuous values)\n",
            "6. Majority vote for final prediction (not averaging)\n",
            "7. Uses water_stress_index to create risk categories\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Line 180-187: Example JSON output for integration\n",
        "example_vector = X_test[0].reshape(1,-1)\n",
        "risk_class = int(rf_sklearn.predict(example_vector)[0])  # Predicted class\n",
        "risk_probs = rf_sklearn.predict_proba(example_vector)[0].tolist()  # Class probabilities\n",
        "print(f\"\\nExample JSON output for risk model:\")\n",
        "print({\"risk_class\": risk_class, \"risk_probs\": risk_probs})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY DIFFERENCES FROM REGRESSION MODEL:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Uses RandomForestClassifier (not Regressor)\")\n",
        "print(\"2. Target is categorical (0, 1, 2) not continuous\")\n",
        "print(\"3. Uses Gini impurity for splits (not MSE)\")\n",
        "print(\"4. Uses accuracy, classification_report (not RMSE, RÂ²)\")\n",
        "print(\"5. Predicts class probabilities (not continuous values)\")\n",
        "print(\"6. Majority vote for final prediction (not averaging)\")\n",
        "print(\"7. Uses water_stress_index to create risk categories\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
